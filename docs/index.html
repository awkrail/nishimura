<!DOCTYPE html>
<html>
	<head>
	<meta name="generator" content="Hugo 0.123.7">
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Taichi Nishimura, Ph.D.- Ph.D. in Informatics</title><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
	<link rel="manifest" href="site.webmanifest">
	<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5">
	<meta name="msapplication-TileColor" content="#da532c">
	<meta name="theme-color" content="#ffffff">

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="alternate" type="application/rss+xml" href="https://awkrail.github.io/nishimura/index.xml" title="Taichi Nishimura, Ph.D." />
	<meta property="og:title" content="Taichi Nishimura, Ph.D." />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://awkrail.github.io/nishimura/" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Taichi Nishimura, Ph.D."/>
<meta name="twitter:description" content=""/>
<link rel="stylesheet" type="text/css" media="screen" href="css/bootstrap.min.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="css/normalize.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="css/main.css" /><script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/main.js"></script>
</head>


	<body>
		<div class="container wrapper">
			<div class="header">
	<img src=https://awkrail.github.io/nishimura/profile.jpg class="profile_image">
	<h1>Taichi Nishimura, Ph.D.</h1>
	<div class="site-affilation">
		<span class="affilation"><ul class="flat">
				<li class="position">Senior Machine Learning Engineer at PlayStation</li>
				<li class="email">taichitary@gmail.com</li>
			</ul></span>
	</div>
</div>
            <h3>Education</h3><ul>
  
  <div class="row biography">
        <div class="col-sm-5">
        October 2020 - September 2023
        </div>
        <div class="col-sm-7">
        <b>Ph.D. in Informatics</b>, Kyoto University
        <br>
        
          Research Fellow (DC1), JSPS Research Fellowships for Young Scientists</div>
  </div>
  
  <div class="row biography">
        <div class="col-sm-5">
        April 2019 - September 2020
        </div>
        <div class="col-sm-7">
        <b>M.S. in Informatics</b>, Kyoto University
        <br>
        </div>
  </div>
  
  <div class="row biography">
        <div class="col-sm-5">
        April 2015 - March 2019
        </div>
        <div class="col-sm-7">
        <b>B.E. in Design</b>, Kyushu University
        <br>
        </div>
  </div>
  
</ul>
			<h3>Work Experience</h3><ul>
  
  <div class="row biography">
        <div class="col-sm-5">
        Feburary 2025 - Present
        </div>
        <div class="col-sm-7">
        <b>Senior Machine Learning Engineer</b>, PlayStation
        </div>
  </div>
  
  <div class="row biography">
        <div class="col-sm-5">
        October 2023 - December 2024
        </div>
        <div class="col-sm-7">
        <b>Computer vision research engineer</b>, LY Corporation
        </div>
  </div>
  
</ul>
			<div class="introduction">
	<h3>Softwares</h3>
	<ul>
		
		<li>
		
		<a href="https://github.com/awkrail/shutoh" class="publications"> Shutoh, </a>
		
		
		A fast scene detector implemented in C&#43;&#43;20. Inspired by PySceneDetect, Shutoh aims to provide a powerful and flexible alternative with enhanced performance. Shutoh achieves 2x~4x faster than PySceneDetect and supports not only rule-based (PySceneDetect), but also ML-based and neural-based scene detectors.
	
		
		</li>
		
		<li>
		
		<a href="https://github.com/line/lighthouse" class="publications"> line/lighthouse, </a>
		
		
		A user-friendly library for reproducible video moment retrieval and highlight detection (MR-HD). It supports six models, three features, and five datasets. In addition, we prepare an inference-only API and demo for developers to use MR-HD methods easily.
	
		
		</li>
		
	</ul></div>

<div class="introduction">
	<h3>Publication</h3>
	
	<h4>International Journal</h4><div class="year"></div>
	<ul>
		
		<li>
		
		<a href="https://arxiv.org/abs/2404.02523" class="publications"> Text-driven Affordance Learning from Egocentric Vision, </a>
		
		Advanced Robotics (minor revision),
	
		
	
		<span class="collaborators">
			Tomoya Yoshida, Shuhei Kurita, <u>Taichi Nishimura</u>, Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2312.00414" class="publications"> Vision-Language Models Learn Super Images for Efficient Partially Relevant Video Retrieval, </a>
		
		ACM Transactions on Multimedia Computing, Communications, and Applications (ACM TOMM),
	
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Shota Nakada, Masayoshi Kondo
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2209.10134" class="publications"> Recipe Generation from Unsegmented Cooking Videos, </a>
		
		ACM Transactions on Multimedia Computing, Communications, and Applications (ACM TOMM),
	
		
		<span class="code_blog">
			
			[<a href="https://github.com/awkrail/jesg">code</a>]
			
	
			
		</span>
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/nishimura-MTAP2023.pdf" class="publications"> State-aware Video Procedural Captioning, </a>
		
		Multimedia Tools and Applications (MTAP) Vol 82 (24),
	
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://ieeexplore.ieee.org/document/9288722" class="publications"> Structure-Aware Procedural Text Generation from an Image Sequence, </a>
		
		IEEE Access Vol. 9,
	
		
		<span class="code_blog">
			
	
			
			[<a href="https://speakerdeck.com/awkrail/structure-aware-procedural-text-generation-from-an-image-sequence?slide=4">slide</a>]
			
		</span>
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, Yoko Yamakata, and Shinsuke Mori
		</span>
		</li>
		
	</ul><h4>International Conference</h4><div class="year"></div>
	<ul>
		
		<li>
		
		<a href="https://arxiv.org/abs/2404.03161" class="publications"> BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro QR Codes, </a>
		
		IEEE International Conference on Image Processing (ICIP25),
	
		
		<span class="code_blog">
			
			[<a href="https://github.com/nishi10mo/BioVLQR_StepLocalization">code</a>]
			
	
			
			[<a href="https://nishi10mo.github.io/BioVL-QR/">slide</a>]
			
		</span>
		
	
		<span class="collaborators">
			Tomohiro Nishimoto, <u>Taichi Nishimura</u>, Koki Yamamoto, Keisuke Shirai, Hirotaka Kameko, Yuto Haneji, Tomoya Yoshida, Keiya Kajimura, Taiyu Cui, Chihiro Nishiwaki, Eriko Daikoku, Natsuko Okuda, Fumihito Ono, Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2506.03605" class="publications"> Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision, </a>
		
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR25),
	
		
		<span class="code_blog">
			
			[<a href="https://github.com/Biscue5/EgoScaler">code</a>]
			
	
			
			[<a href="https://biscue5.github.io/egoscaler-project-page">slide</a>]
			
		</span>
		
	
		<span class="collaborators">
			Tomoya Yoshida, Shuhei Kurita, <u>Taichi Nishimura</u>, Shinsuke Mori <font color="red"> (Highlight)</font>
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2409.15672" class="publications"> Language-based Audio Moment Retrieval, </a>
		
		IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP25),
	
		
		<span class="code_blog">
			
			[<a href="https://github.com/line/lighthouse">code</a>]
			
	
			
			[<a href="https://h-munakata.github.io/Language-based-Audio-Moment-Retrieval/">slide</a>]
			
		</span>
		
	
		<span class="collaborators">
			Hokuto Munakata, <u>Taichi Nishimura</u>, Shota Nakada, Tatsuya Komatsu
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2409.11729" class="publications"> DETECLAP: Enhancing Audio-Visual Representation Learning with Object Information, </a>
		
		IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP25),
	
		
	
		<span class="collaborators">
			Shota Nakada, <u>Taichi Nishimura</u>, Hokuto Munakata, Tatsuya Komatsu
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2311.16444" class="publications"> Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos, </a>
		
		IEEE/CVF Winter Conference on Applications of Computer Vision (WACV25),
	
		
	
		<span class="collaborators">
			Takehiko Ohkawa, Takuma Yagi, <u>Taichi Nishimura</u>, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, Yoichi Sato
		</span>
		</li>
		
		<li>
		
		<a href="http://arxiv.org/abs/2408.02901" class="publications"> Lighthouse: A User-Friendly Library for Reproducible Video Moment Retrieval and Highlight Detection, </a>
		
		The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP24) - System Demonstration Track,
	
		
		<span class="code_blog">
			
			[<a href="https://github.com/line/lighthouse">code</a>]
			
	
			
		</span>
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu
		</span>
		</li>
		
		<li>
		
		<a href="https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Munakata_60.pdf" class="publications"> Pre-trained models, Datasets, Data Augmentation, and Inference Time Augmentation for Language-based Audio Retrieval, </a>
		
		IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events Workshop (DCASE24 Workshop),
	
		
	
		<span class="collaborators">
			Hokuto Munakata, <u>Taichi Nishimura</u>, Shota Nakada, Tatsuya Komatsu
		</span>
		</li>
		
		<li>
		
		<a href="https://dcase.community/documents/challenge2024/technical_reports/DCASE2024_Munakata_45_t8.pdf" class="publications"> Training Strategy of Massive Text-to-audio Models and GPT-based Query-Augmentation, </a>
		
		IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events Challenge Task 8: Language-Based Audio Retrieval (DCASE24 Challenge),
	
		
	
		<span class="collaborators">
			Hokuto Munakata, <u>Taichi Nishimura</u>, Shota Nakada, Tatsuya Komatsu
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2311.16444" class="publications"> Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos, </a>
		
		The 1st Workshop on Learning from Procedural Videos and Language in conjunction with CVPR2024 (LPVL24),
	
		
	
		<span class="collaborators">
			Takehiko Ohkawa, Takuma Yagi, <u>Taichi Nishimura</u>, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, Yoichi Sato
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2403.16483" class="publications"> Automatic Construction of a Large-Scale Corpus for Geoparsing Using Wikipedia Hyperlinks, </a>
		
		The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING2024),
	
		
	
		<span class="collaborators">
			Keyaki Ohno, Hirotaka Kameko, Keisuke Shirai, <u>Taichi Nishimura</u> and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2209.05840" class="publications"> Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows, </a>
		
		The 29th International Conference on Computational Linguistics (COLING2022),
	
		
		<span class="code_blog">
			
			[<a href="https://github.com/kskshr/Visual-Recipe-Flow">code</a>]
			
	
			
		</span>
		
	
		<span class="collaborators">
			Keisuke Shirai, Atsushi Hashimoto, <u>Taichi Nishimura</u>, Hirotaka Kameko, Shuhei Kurita, Yoshitaka Ushiku and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://dl.acm.org/doi/abs/10.1145/3552485.3554934" class="publications"> Multimodal Dish Pairing: Predicting Side Dishes to Serve with a Main Dish, </a>
		
		The 1st International Workshop on Multimedia for Cooking, Eating, and related APPlications 2022 in conjunction with ACMMM2022 (CEA++2022),
	
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Katsuhiko Ishiguro, Keita Higuchi, and Masaaki Kotera <font color="red"> (Best Paper Award)</font>
		</span>
		</li>
		
		<li>
		
		<a href="https://dl.acm.org/doi/abs/10.1145/3552485.3554941" class="publications"> Recipe Recommendation for Balancing Ingredient Preference and Daily Nutrients, </a>
		
		The 1st International Workshop on Multimedia for Cooking, Eating, and related APPlications 2022 in conjunction with ACMMM2022 (CEA++2022),
	
		
	
		<span class="collaborators">
			Sara Ozeki, Masaaki Kotera, Katsuhiko Ishiguro, <u>Taichi Nishimura</u>, Keita Higuchi
		</span>
		</li>
		
		<li>
		
		<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/zhu-DH2022.pdf" class="publications"> Multimedia Retrieval of Historical Materials, </a>
		
		Digital Humanities 2022 (DH2022),
	
		
	
		<span class="collaborators">
			Jieyong Zhu, <u>Taichi Nishimura</u>, Makoto Goto, and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/tanaka-LREC22.pdf" class="publications"> Image Description Dataset for Language Learners, </a>
		
		The 13th International Conference on Language Resources and Evaluation (LREC2022),
	
		
	
		<span class="collaborators">
			Kento Tanaka, <u>Taichi Nishimura</u>, Hiroaki Nanjo, Keisuke Shirai, Hirotaka Kameko, and Masatake Dantsuji
		</span>
		</li>
		
		<li>
		
		<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/nishimura-HCII2022.pdf" class="publications"> Cross-modal Representation Learning for Understanding Manufacturing Procedure, </a>
		
		The 23th International Conference on Human-Computer Interaction (HCII2022),
	
		
	
		<span class="collaborators">
			Atsushi Hashimoto, <u>Taichi Nishimura</u>, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475322" class="publications"> State-aware Video Procedural Captioning, </a>
		
		The 29th ACM International Conference on Multimedia (ACMMM2021),
	
		
		<span class="code_blog">
			
			[<a href="https://github.com/awkrail/svpc">code</a>]
			
	
			
		</span>
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Nishimura_Egocentric_Biochemical_Video-and-Language_Dataset_ICCVW_2021_paper.pdf" class="publications"> Egocentric Biochemical Video-and-Language Dataset, </a>
		
		The 4th Workshop on Closing the Loop Between Vision and Language in conjunction with ICCV2021 (CLVL2021),
	
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Kojiro Sakoda, Atsushi Hashimoto, Yoshitaka Ushiku, Natsuko Tanaka, Fumihito Ono, Hirotaka Kameko, and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://www.aclweb.org/anthology/2020.lrec-1.527.pdf" class="publications"> Visual Grounding Annotation of Recipe Flow Graph, </a>
		
		The 12th International Conference on Language Resources and Evaluation (LREC2020),
	
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Suzushi Tomori, Hayato Hashimoto, Atsushi Hashimoto, Yoko Yamakata, Jun Harashima, Yoshitaka Ushiku and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://aclanthology.org/W19-8650/" class="publications"> Procedural Text Generation from a Photo Sequence, </a>
		
		The 12th International Conference on Natural Language Generation (INLG2019),
	
		
		<span class="code_blog">
			
	
			
			[<a href="https://drive.google.com/file/d/10z_cFHy3O_zqjRF7fAAKjW7MN3AREbqH/view?usp=sharing">slide</a>]
			
		</span>
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Atsushi Hashimoto, and Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://dl.acm.org/doi/10.1145/3326458.3326928" class="publications"> Frame Selection for Producing Recipe with Pictures from an Execution Video of a Recipe, </a>
		
		The 11th Workshop on Multimedia for Cooking and Eating Activities in conjunction with ICMR2019 (CEA2019),
	
		
		<span class="code_blog">
			
	
			
			[<a href="https://speakerdeck.com/awkrail/frame-selection-for-producing-recipe-with-pictures-from-an-execution-video-of-a-recipe">slide</a>]
			
		</span>
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Atsushi Hashimoto, Yoko Yamakata, and Shinsuke Mori <font color="red"> (Best Paper Award)</font>
		</span>
		</li>
		
	</ul>
	<h4>Preprint</h4><div class="year"></div>
	<ul>
		
		<li>
		
		<a href="https://arxiv.org/abs/2410.05343" class="publications"> EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos with Procedural Texts, </a>
		
		arXiv:2410.05343,
	
		
		<span class="code_blog">
			
			[<a href="https://github.com/Y-Haneji/EgoOops-annotations/">code</a>]
			
	
			
			[<a href="https://y-haneji.github.io/EgoOops-project-page/">slide</a>]
			
		</span>
		
	
		<span class="collaborators">
			Yuto Haneji, <u>Taichi Nishimura</u>, Hirotaka Kameko, Keisuke Shirai, Tomoya Yoshida, Keiya Kajimura, Koki Yamamoto, Taiyu Cui, Tomohiro Nishimoto, Shinsuke Mori
		</span>
		</li>
		
		<li>
		
		<a href="https://arxiv.org/abs/2401.09774" class="publications"> On the Audio Hallucinations in Large Audio-Video Language Models, </a>
		
		arXiv:2401.09774,
	
		
	
		<span class="collaborators">
			<u>Taichi Nishimura</u>, Shota Nakada, Masayoshi Kondo
		</span>
		</li>
		
	</ul>
	<h4>Domestic Journal and Conference (Japanese)</h4>
	<details>
		<summary>Please click here</summary><div class="year"></div>
		<ul>
			
			<li>
			
			<p class="publications">BioVL-QR: マイクロQRコードを用いた生化学分野の一人称視覚言語データセット,</p>
			
			第28回 画像の認識・理解シンポジウム (MIRU2025).
		
			
		
			<span class="collaborators">
				西本 智裕, <u>西村 太一</u>, 山本 航輝, 白井 圭佑, 亀甲 博貴, 羽路 悠斗, 吉田 智哉, 梶村 恵矢, 崔 泰毓, 西脇 千紘, 大黒 恵理子, 奥田 奈津子, 小野 富三人, 森 信介
			</span>
			</li>
			
			<li>
			
			<p class="publications">ICASSP2025における音響-言語モデルの動向,</p>
			
			電気音響研究会 / 応用音響研究会.
		
			
		
			<span class="collaborators">
				宗像 北斗, <u>西村 太一</u>, 仲田 勝太, 小松 達也
			</span>
			</li>
			
			<li>
			
			<p class="publications">音響特徴量を活用した動画内区間検索及びハイライト検出,</p>
			
			第152回 日本音響学会 (ASJ2024).
		
			
		
			<span class="collaborators">
				今村 剛大, <u>西村 太一</u>, 小松 達也, 戸田 智基
			</span>
			</li>
			
			<li>
			
			<p class="publications">自然言語による音響区間検索,</p>
			
			第152回 日本音響学会 (ASJ2024).
		
			
		
			<span class="collaborators">
				宗像 北斗, <u>西村 太一</u>, 仲田 勝太, 小松 達也
			</span>
			</li>
			
			<li>
			
			<p class="publications">Lighthouse: 再現可能で使いやすい動画区間検索のライブラリ,</p>
			
			第27回 画像の認識・理解シンポジウム (MIRU2024).
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>
			</span>
			</li>
			
			<li>
			
			<p class="publications">音を発生させる物体を考慮した視聴覚表現学習,</p>
			
			第27回 画像の認識・理解シンポジウム (MIRU2024).
		
			
		
			<span class="collaborators">
				仲田 勝太, <u>西村 太一</u>, 宗像 北斗, 小松 達也, 近藤 雅芳
			</span>
			</li>
			
			<li>
			
			<p class="publications">動画生成における文字崩れの評価のためのデータセットと評価指標の提案,</p>
			
			第27回 画像の認識・理解シンポジウム (MIRU2024).
		
			
		
			<span class="collaborators">
				青嶋 雄大, <u>西村 太一</u>, 近藤 雅芳
			</span>
			</li>
			
			<li>
			
			<a href="https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P7-15.pdf" class="publications"> 一人称視点に基づくテキスト駆動型アフォーダンス及び軌跡の学習, </a>
			
			言語処理学会第30回年次大会 (NLP2024).
		
			
		
			<span class="collaborators">
				吉田 智哉, 栗田 修平, <u>西村 太一</u>, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P7-14.pdf" class="publications"> EgoOops!データセット: 手順書に従う作業の一人称映像への作業誤りアノテーション, </a>
			
			言語処理学会第30回年次大会 (NLP2024).
		
			
		
			<span class="collaborators">
				羽路 悠斗, <u>西村 太一</u>, 山本 航輝, 梶村 恵矢, 崔 泰毓, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P7-9.pdf" class="publications"> 一人称視点動画を用いたマルチモーダル作業支援システム, </a>
			
			言語処理学会第30回年次大会 (NLP2024).
		
			
		
			<span class="collaborators">
				梶村 恵矢, <u>西村 太一</u>, 羽路 悠斗, 山本 航輝, 崔 泰毓, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://www.wiss.org/WISS2023Proceedings/data/3-A01.pdf" class="publications"> 一人称視点動画を用いたマルチモーダル作業支援システムの提案, </a>
			
			第31回インタラクティブシステムとソフトウェアに関するワークショップ (WISS2023).
		
			
		
			<span class="collaborators">
				梶村 恵矢, <u>西村 太一</u>, 羽路 悠斗, 山本 航輝, 崔 泰毓, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://www.jstage.jst.go.jp/article/jnlp/30/3/30_1042/_pdf/-char/ja" class="publications"> 調理動作後の物体の視覚的状態予測を目指したVisual Recipe Flowデータセットの構築と評価, </a>
			
			自然言語処理 Vol 30 (3).
		
			
		
			<span class="collaborators">
				白井 圭佑, 橋本 敦史, <u>西村 太一</u>, 亀甲 博貴, 栗田 修平, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://www.jstage.jst.go.jp/article/jnlp/30/2/30_833/_pdf/-char/ja" class="publications"> 「BioVL2データセット: 生化学分野における一人称視点の実験映像への言語アノテーション」の研究経緯, </a>
			
			自然言語処理.
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/yagi-NLP2023.pdf" class="publications"> 大規模言語モデルからの知識抽出に基づく画像からのスクリプト予測の検討, </a>
			
			言語処理学会第29回年次大会 (NLP2023).
		
			
		
			<span class="collaborators">
				八木 拓真, <u>西村 太一</u>, 清丸 寛一, 唐井 希
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/kinoshita-NLP2023.pdf" class="publications"> 株式投資家の関心を考慮したニュース記事抽出によるストーリー生成, </a>
			
			言語処理学会第29回年次大会 (NLP2023).
		
			
		
			<span class="collaborators">
				木下 聖, <u>西村 太一</u>, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/ohno-NLP2023.pdf" class="publications"> テキスト中の場所表現認識と係り受けに基づく緯度経度推定ツールの開発, </a>
			
			言語処理学会第29回年次大会 (NLP2023).
		
			
		
			<span class="collaborators">
				大野 けやき, <u>西村 太一</u>, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/yamamoto-NLP2023.pdf" class="publications"> VideoCLIPを用いた実験動画からのプロトコル生成, </a>
			
			言語処理学会第29回年次大会 (NLP2023).
		
			
		
			<span class="collaborators">
				山本 航輝, <u>西村 太一</u>, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/yoshida-NLP2023.pdf" class="publications"> 単語の階層関係に基づくデータ拡張を利用した画像キャプション生成の検討, </a>
			
			言語処理学会第29回年次大会 (NLP2023).
		
			
		
			<span class="collaborators">
				吉田 智哉, <u>西村 太一</u>, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/morita-NLP2023.pdf" class="publications"> テキストアナリティクスツールのログからの実験設定の説明文生成, </a>
			
			言語処理学会第29回年次大会 (NLP2023).
		
			
		
			<span class="collaborators">
				森田 康介, <u>西村 太一</u>, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/nishimura_nlp_journal_2022-3.pdf" class="publications"> BioVL2データセット: 生化学分野における一人称視点の実験映像への言語アノテーション, </a>
			
			自然言語処理 Vol 29 (4).
		
			
			<span class="code_blog">
				
				[<a href="https://github.com/awkrail/BioVL2">code</a>]
				
		
				
			</span>
			
		
			<span class="collaborators">
				<u>西村 太一</u>, 迫田 航次郎, 牛久 敦, 橋本 敦史, 奥田 奈津子, 小野 富三人, 亀甲 博貴, 森 信介 <font color="red"> (論文賞)</font>
			</span>
			</li>
			
			<li>
			
			<a href="https://www.wiss.org/WISS2022Proceedings/data/10.pdf" class="publications"> ユーザ嗜好と栄養摂取基準に基づくレシピ推薦手法の開発, </a>
			
			第30回インタラクティブシステムとソフトウェアに関するワークショップ (WISS2022).
		
			
		
			<span class="collaborators">
				尾関 沙羅, 小寺 正明, 石黒 勝彦, <u>西村 太一</u>, 樋口 啓太
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/publications/morita-NL2022.pdf" class="publications"> テキストマイニングツールのログからの実験設定の説明文生成, </a>
			
			第253回自然言語処理研究会 (NL253).
		
			
		
			<span class="collaborators">
				森田 康介, <u>西村 太一</u>, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/nishimura-NLP22.pdf" class="publications"> 映像からのストーリー生成: イベント選択器と文生成器の同時学習, </a>
			
			言語処理学会第28回年次大会 (NLP2022).
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>, 橋本 敦史, 牛久 祥孝, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/sakoda-NLP22.pdf" class="publications"> 生化学分野におけるVideo&amp;Languageデータセットの構築, </a>
			
			言語処理学会第28回年次大会 (NLP2022).
		
			
		
			<span class="collaborators">
				迫田 航次郎, <u>西村 太一</u>, 森 信介, 小野 富三人, 田中 奈津子
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/hoshijima-NLP22.pdf" class="publications"> 市民科学でのアノテーション作業支援と作業者の能力向上支援, </a>
			
			言語処理学会第28回年次大会 (NLP2022).
		
			
		
			<span class="collaborators">
				星島 洸明, <u>西村 太一</u>, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/tanaka-NLP22.pdf" class="publications"> 画像描写問題における学習者作文の訂正文生成, </a>
			
			言語処理学会第28回年次大会 (NLP2022).
		
			
		
			<span class="collaborators">
				田中 健斗, <u>西村 太一</u>, 南條 浩輝, 白井 圭佑, 亀甲 博貴
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/zhu-NLP22.pdf" class="publications"> Cross-modal Retrieval of Historical Materials, </a>
			
			言語処理学会第28回年次大会 (NLP2022).
		
			
		
			<span class="collaborators">
				Jieyong Zhu, <u>Taichi Nishimura</u>, Makoto Goto, Shinsuke Mori
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/tanaka-jsai21.pdf" class="publications"> 写真描画問題における自動採点手法の検討, </a>
			
			2021年度 人工知能学会全国大会 (JSAI2021).
		
			
		
			<span class="collaborators">
				田中 健斗, <u>西村 太一</u>, 白井 圭佑, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/nishimura-NLP21.pdf" class="publications"> 手順構造を考慮した作業映像からの手順書生成, </a>
			
			言語処理学会第27回年次大会 (NLP2021).
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>, 橋本 敦史, 牛久 祥孝, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/sakoda-NLP21.pdf" class="publications"> 手順構造を考慮した手順書からの作業画像検索, </a>
			
			言語処理学会第27回年次大会 (NLP2021).
		
			
		
			<span class="collaborators">
				迫田 航次郎, <u>西村 太一</u>, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/mori/research/public/hoshijima-NLP21.pdf" class="publications"> 複数作業者を想定したアノテーションツールの作成と機能の検討, </a>
			
			言語処理学会第27回年次大会 (NLP2021).
		
			
		
			<span class="collaborators">
				星島 洸明, <u>西村 太一</u>, 亀甲 博貴, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://ict-nw.i.kyoto-u.ac.jp/ict-innovation/panel/407/" class="publications"> 手順構造を考慮した写真列からの手順書生成, </a>
			
			京都大学ICTイノベーション.
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>
			</span>
			</li>
			
			<li>
			
			<a href="https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/A1-3.pdf" class="publications"> 写真列と構造要素からの手順構造と手順書の同時学習, </a>
			
			言語処理学会第26回年次大会 (NLP2020).
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>, 橋本 敦史, 牛久 祥孝, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/A1-2.pdf" class="publications"> レシピフローグラフへのVisual Groundingアノテーション, </a>
			
			言語処理学会第26回年次大会 (NLP2020).
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>, 友利 涼, 橋本 隼人, 橋本 敦史, 山肩 洋子, 原島 純, 牛久 祥孝, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://www.jstage.jst.go.jp/article/jnlp/27/2/27_257/_pdf/-char/en" class="publications"> 重要語に着目した写真列からのレシピの自動生成, </a>
			
			自然言語処理 Vol. 27 (2).
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>, 橋本 敦史, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://www.ieice.org/ken/paper/2019102641Qi/" class="publications"> 作業写真列からの手順書の自動生成, </a>
			
			ヒューマンコミュニケーション基礎研究会 (HCS).
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>, 橋本 敦史, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="http://www.lsta.media.kyoto-u.ac.jp/member/nishimura/papers/yans2019/poster.pdf" class="publications"> Bounding Boxを付与したフローグラフコーパスの提案, </a>
			
			自然言語処理若手の会 (yans2019).
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>, 橋本 敦史, 原島 純, 山肩 洋子, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://db-event.jpn.org/deim2019/post/papers/200.pdf" class="publications"> 写真付き手順書生成のための実施映像からのフレーム選択, </a>
			
			第11回データ工学と情報マネジメントに関するフォーラム (DEIM2019).
		
			
		
			<span class="collaborators">
				<u>西村 太一</u>, 橋本 敦史, 山肩 洋子, 森 信介
			</span>
			</li>
			
			<li>
			
			<a href="https://confit.atlas.jp/guide/event/vrsj2017/subject/1E3-04/detail" class="publications"> テキスタイルセンサを用いた腹巻型笑いログシステムによる笑い検出の検討, </a>
			
			第22回日本バーチャルリアリティ学会 (VRSJ2017).
		
			
		
			<span class="collaborators">
				島﨑 郁花, <u>西村 太一</u>, 上岡 玲子
			</span>
			</li>
			
		</ul></details>
</div>
			<div class="talk">
	<h3>News and Invited Talk</h3><ul>
		
		<li>
		
		<a href="https://speakerdeck.com/awkrail/zuo-ye-dong-hua-toshou-shun-shu-wodui-xiang-tositamarutimodaruli-jie" class="content"> 作業動画と手順書を対象としたマルチモーダル理解, </a>
		
		Vision and Languageの最前線, 第26回情報論的学習理論ワークショップ (IBIS2023)
	
		
	
		<span class="collaborators">
			
		</span>
		</li>
		
		<li>
		
		<a href="https://www.nikkei.com/article/DGKKZO65214050X11C22A0TEB000/" class="content"> オムロン系、料理動画→レシピ作成 AIが自動で、音声・字幕なしでも, </a>
		
		日本経済新聞
	
		
	
		<span class="collaborators">
			
		</span>
		</li>
		
	</ul></div>
<h3>Academic Activities</h3>
<b>Journal reviewer</b>: IEEE Transactions on Multimedia, Advanced Robotics, Journal of Natural Language Processing
<br>
<b>Conference reviewer</b>: WACV, ECCV, LREC
<br>
<br>
		</div>
	</body>

</html>
